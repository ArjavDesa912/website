# Robots.txt for Praesidium Systems
# https://praesidiumsystems.ai/robots.txt

# Allow all web crawlers to access all content
User-agent: *
Allow: /

# Specific instructions for major search engines
User-agent: Googlebot
Allow: /
Crawl-delay: 1

User-agent: Bingbot
Allow: /
Crawl-delay: 1

User-agent: Slurp
Allow: /
Crawl-delay: 2

# Disallow access to sensitive areas
Disallow: /admin/
Disallow: /api/internal/
Disallow: /config/
Disallow: /.env
Disallow: /node_modules/
Disallow: /src/
Disallow: /build/
Disallow: /*.json$
Disallow: /*.log$

# Disallow duplicate content parameters
Disallow: /*?utm_*
Disallow: /*?ref=*
Disallow: /*?source=*
Disallow: /*?campaign=*

# Allow specific important files
Allow: /sitemap.xml
Allow: /robots.txt
Allow: /favicon.ico
Allow: /.well-known/

# Block AI training crawlers (optional - many companies are doing this)
User-agent: ChatGPT-User
Disallow: /

User-agent: GPTBot
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

# Sitemap location
Sitemap: https://praesidiumsystems.ai/sitemap.xml

# Additional sitemaps (if you create them)
# Sitemap: https://praesidiumsystems.ai/sitemap-blog.xml
# Sitemap: https://praesidiumsystems.ai/sitemap-products.xml

# Host directive (helps with canonicalization)
Host: https://praesidiumsystems.ai